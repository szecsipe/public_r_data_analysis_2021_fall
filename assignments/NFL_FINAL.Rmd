---
title: "NFL stats"
author: "Szécsi Péter"
date: "1/16/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(tidyverse)
library(ggfortify)
library(broom)
```
# Introduction

In the chosen data table you can find statistics about NFL players. Every row contains data about one player in one match. My goal in this analysis is to create two plots and two regression models. To decide what my hypotheses and interest should be, lets explore the data!

### Read the file
```{r}
raw <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-08-28/nfl_2010-2017.csv")[,-1]
```


```{r}
head(raw)
```

Looks like it was read in properly.
```{r}
str(raw)
```

Position and team could be factor but otherwise all is fine.


## Data exploration and data cleaning

First I will check whether the data is what I think it is (variables look as they should look).

Date:
```{r}
hist(raw$game_year)
```

There is a bump in the first column, lets look at the number of rows for each year.
```{r}
as.data.frame(table(raw$game_year))
```

In the table you can see it is not problematic, probably the low number of breaks caused the problem, lets look at the histogram again, this time with more breaks in it.
```{r}
hist(raw$game_year, 25)
```

Now it looks normal.

### Teams
```{r}
teams <- as.data.frame(table(raw$team))
view(teams)
```

There are more teams than there are currently in the NFL due to name changes (SD is LAC now; STL is LA now; OAK is LV today (I don't need to change if because they changed their name after 2017, but I do it anyway); JAC is the same as JAX, only the abb. changed)
Lets change it accordingly.
```{r}
raw <- raw %>%
  mutate(team = as.factor(case_when(team == "SD" ~ "LAC",
                                    team == "STL" ~ "LA",
                                    team == "OAK" ~ "LV",
                                    team == "JAC" ~ "JAX",
                                    TRUE ~ team)
                          )
         )
teams <- as.data.frame(table(raw$team))
view(teams)
```

### Name

I look for reading in mistakes, lets print the longest and shortest names to see if something stuck together.
```{r}
raw[which.max(nchar(raw$name)), "name"]
raw[which.min(nchar(raw$name)), "name"]
```

Looks great.

### Position
```{r}
as.data.frame(table(raw$position))
```

There are three different positions in the data table. There is enough data to analyze any position, I will check the corresponding performance metrics once I pick the position of interest.

### Rate
The variable which can be used as an outcome measure the best is rate (average ratings of the player in the match).

First I count the missing values
```{r}
sum(is.na(raw$rate))
```

Then I check its distribution.
```{r}
hist(raw$rate, 1000)
```

There is a curiously high column at 0 and somewhere around 40. I will need to investigate it later.
The number of missing data is high (71044). I check the number of data from each position after excluding the data with missing rating value.
```{r}
rated <- raw %>%
  filter(is.na(rate) == FALSE)

as.data.frame(table(rated$position))
```

All of the QBs have ratings. I check whether the key performance metrics belonging to QBs are filled out.
```{r}
qb <- rated %>%
  filter(position == "QB")
```

Number of pass attempts [Missing]
```{r}
sum(is.na(qb$pass_att))
```

Passed yards [Missing]
```{r}
sum(is.na(qb$pass_yds))
```

Number of touchdown passes [Missing]
```{r}
sum(is.na(qb$pass_tds))
```

Number of interceptions (when the pass is stolen by the opposing team) [Missing]
```{r}
sum(is.na(qb$int))
```

Number of times the QB is tackled while having a ball in his hand (usually does not count as his fault) [Missing]
```{r}
sum(is.na(qb$sck))
```

Number of times the ball gets stolen shortly before the QB releases the ball suring the passing movement [Missing]
```{r}
sum(is.na(qb$pass_fumbles))
```

The most important metrics contain 0 missing value which is great. There are some less important coloumns, but for some QBs they should be accounted for (depends on the player's style)

#Number of rush attempts [Missing]
```{r}
sum(is.na(qb$rush_att))
```

Rushed yards [Missing]
```{r}
sum(is.na(qb$rush_yds))
```

Number of touchdown rushes [Missing]
```{r}
sum(is.na(qb$rush_tds))
```

AVerage rush distance [Missing]
```{r}
sum(is.na(qb$rush_avg))
```

Number of times the ball gets stolen mid rush [Missing]
```{r}
sum(is.na(qb$rush_fumbles))
```

The number of missing values is the same in every case (1444) so probably they are missing in the same rows and I can exclude them without losing most of my data. I choose to exclude those rows to get a full picture about the QBs in my analysed sample. Lets see how many rows remain in my dataset.
```{r}
#Choose cols if interest
qb <- qb %>%
  select(1:9, 15:21)

#Choose complete cases
qb <- qb[complete.cases(qb),]

nrow(qb)
```

More rows disappeared, but every remaining column is complete, and the number of coloumns is still enough to analysie.
I check the distribution of the ratings again to investigate the curiously high coloumns.
```{r}
hist(qb$rate, 1000)
```

It looks much better, but I will need exclude the outliers (there are spikes at the end).

I use the 1.5*IQR criterion above and below the 3rd and 1st quartiles accordingly.
Lets see if it changed the distribution.
```{r}
qb_trimmed <- qb %>%
  filter(rate > quantile(qb$rate)[[2]] - 1.5*IQR(qb$rate) & 
         rate < quantile(qb$rate)[[3]] + 1.5*IQR(qb$rate))
hist(qb_trimmed$rate, 1000)
```

The distribution has a thick tale approaching 0, but otherwise looks great.

# Research question
In this analysis I intend to explore what performance metrics can efficiently predics the ratings of the Quarterbacks (QBs). This question is not unique probably, because QBs are in the focus of the game and many fans think that the QB is the player who is mainly responsible for the success of the team. Sadly I can not test that hypothesis since I have no data available about the outcome of the games. I choose my research question to help the QBs of tomorrow. If they know what metrics (skills) they have to pay attention to have a higher rating they will have a greater chance to be drafted or be the crowd favorite.

First, I will build a model (m1) that contains only the key QB metrics (pass_yds, pass_tds, int). Second, I will build a model (m2) that contains the metrics that is contained by m1 and I add the metrics hybrid players are known for (rush_yds, rush_tds, rush_fumbles).

# Plotting
Before I start to test my hypotheses I produce two plots to know more about the QBs of these years.

First, I will check who were the most successful QBs in each team (I will only count those players who played at least 16 matches, which is the length of the regular season)

```{r}
MVP_dat <- qb_trimmed %>%
  group_by(team, name) %>%
  summarize(Rating = mean(rate), Matches = n(), SD = sd(rate)) %>%
  filter(Matches >=16) %>%
  group_by(team) %>%
  filter(Rating == max(Rating))

MVP <- ggplot(MVP_dat, aes(x = Rating, y = team)) +
  geom_text(aes(label = name), nudge_y = -0.5) +
  geom_point()+
  theme_light() +
  theme(panel.border = element_blank(), plot.title = element_text(hjust=0.5)) +
  xlim(82, 105) +
  labs(y = "",
       x = "Average ratings of preformance during his time on the team") +
  coord_cartesian(ylim = c(0,31)) +
  ggtitle(expression(atop("The Rating of the Highest Rated Player of each Franchise",
                    paste("(With a QB in the database who played at least 16 matches)"))))
MVP
```

The plot shows us that it does not mean the same to be the best in every team. You can even see that Peyton Manning managed to be the highest rated players of two teams (Denver and Indianapolis). Standard deviations of ratings were not added to the plot (I plot it below), they show that it is possible that these players do not differ statistically in their ratings.

```{r}
MVP_bars <- ggplot(MVP_dat, aes(x = Rating, y = team)) +
  geom_text(aes(label = name), nudge_y = -0.4) +
  geom_point()+
  theme_light() +
  geom_errorbarh(aes(xmax = Rating + SD, xmin = Rating - SD)) +
  theme(panel.border = element_blank(), plot.title = element_text(hjust=0.5)) +
  xlim(50, 150) +
  labs(y = "",
       x = "Average ratings of preformance during his time on the team with error bars displaying one sd") +
  coord_cartesian(ylim = c(0,31)) +
  ggtitle(expression(atop("The Rating of the Highest Rated Player of each Franchise",
                    paste("(With a QB in the database who played at least 16 matches)"))))
MVP_bars
```

# Pass attempts/passed yards efficiency

The four most successful NFL teams are the Pittsburgh Steelers (PIT), the New England Patriots (NE), the San Francisco 49ers (SF), and the Dallas Cowboys (DAL). Lets look at the performance of those teams' QBs. It would be interesting to look at how efficiently those teams can use a pass attempt. This can be and indicator of how good they are at protecting the QB, catching the ball and creating safe receiving positions.


```{r}
top <- qb_trimmed %>%
  filter(team == "PIT" |
         team == "NE" |
         team == "SF" |
         team == "DAL")


top_pass <- ggplot(top, aes(x = pass_att, y = pass_yds, colour = team)) +
  geom_jitter()+
  theme_light() +
  theme(panel.border = element_blank(), plot.title = element_text(hjust=0.5)) +
  labs(y = "Passed yards",
       x = "Pass attempts") +
  ggtitle(expression(atop("The Rating of the Highest Rated Player of each Franchise",
                    paste("(With a QB in the database who played at least 16 matches)"))))
top_pass
```

There is no obvious pattern that would indicate difference between the teams. If there is difference, it is more subtle. Let's look at a boxplot showing the ratio of pass attempts and passed yards across the four top team.

```{r}
top <- top %>%
  mutate(success = pass_yds/pass_att)


top_success <- ggplot(top, aes(x = team, y = success)) +
  geom_boxplot()+
  theme_light() +
  theme(panel.border = element_blank(), plot.title = element_text(hjust=0.5)) +
  labs(y = "Ratio of the number of attempted passes and passed yards",
       x = "") +
  ggtitle("Ratio of attempted passes and passed yards in the historically top 4 teams") +
  scale_y_continuous(breaks = seq(0, 15, by = 2))
top_success
```

Indeed, based on the boxplots there is no difference between the teams.


To get familiar with the variables I will use I look at some of the relationships between them. First, I check whether the number of attempted passes and the attempted rushes have a inverse relationship.

```{r}
pass_rush <- ggplot(qb_trimmed, aes(x = rush_att, y = pass_att)) +
  geom_jitter() +
  geom_smooth()+
  theme_light() +
  theme(panel.border = element_blank(), plot.title = element_text(hjust=0.5)) +
  labs(y = "Pass attempts",
       x = "Rush attempts") +
  ggtitle("Relationship between the number of attempted passes and rushes")
pass_rush
```

Based on the plot we can say that there might be a weak relationship.

```{r}
cor1 <- cor.test(qb$rush_att, qb$pass_att, method = "spearman")
tidy(cor1)
```

The relationship is weak, almost non-existent.


Secondly, I will check if those who take more risks and pass more interceptions, pass more yards.

```{r}
int_yds <- ggplot(qb_trimmed, aes(x = int, y = pass_yds)) +
  geom_jitter() +
  theme_light() +
  theme(panel.border = element_blank(), plot.title = element_text(hjust=0.5)) +
  labs(y = "Passed yards",
       x = "Number of interceptions") +
  ggtitle("Relationship between the number of passed yards and the number of interceptions")
int_yds
```


```{r}
cor2 <- cor.test(qb$pass_yds, qb$int, method = "pearson")
tidy(cor2)
```

No relationship here either.

# Model fitting

## Complex model

As mentioned above in this section I will try to predict the rating of each players rating using the performance metrics. First, I will fit the more complex model.

```{r}
m2 <- lm(rate ~ pass_yds + pass_tds + int + rush_yds + rush_tds + rush_fumbles, data = qb_trimmed)
tidy(m2)
```

The results of the model make sense, but I will talk about the results once I checked the assumptions.

### Overall picture

```{r}
autoplot(m2)
```

There are problems, Residuals vs Fitted and Q-Q plots and Scale-Location are not great

### Checking for influential outliers

I check the cook distances to look for data that might infulence our model a little too much.
```{r}
cooksd <- cooks.distance(m2)
plot(cooksd, pch="*", cex=2, main="Influential Obs by Cooks distance")
abline(h = 0.0010, col="red") #
text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>0.0010, names(cooksd),""), col="red")
```

Based on the plot I use 0.0010 as cutoff point.

```{r}
influential <- as.numeric(names(cooksd)[(cooksd > (0.0010))])
qb_filtered <- qb_trimmed[-influential, ]
```

Lets check whether the exclusion changed anything

```{r}
m2_2 <- lm(rate ~ pass_yds + pass_tds + int + rush_yds + rush_tds + rush_fumbles, data = qb_filtered)
autoplot(m2_2)
```

Each plot improved, I will continue to use this data


### Check the normality assumption.

```{r}
plot(m2_2, 2)
```

The Q-Q plot looks ok, It follows the line

### Linearity assumption

```{r}
plot(m2_2, 1)
```

There is a little difference between the red line and the middle line, and there seems to be a small trend in the outliers but it is not a great difference.

### Homoscedasticty

```{r}
plot(m2_2, 3)
lmtest::bptest(m2_2)
#the plot is showing a weak curve but based on the statistical test I choose not to worry about it
```

### Multicollinearity

```{r}
as.data.frame(car::vif(m2_2))
```

No values are above 3, so I can say that my variables are independent of each other enough.


## Simple model

I am done with the first model, I will fit my less complex model on the filtered data

```{r}
m1 <- lm(rate ~ pass_yds + pass_tds + int, data = qb_filtered)
tidy(m1)
```

### Overall picture

```{r}
autoplot(m1)
```

These plots are very similiar to the more complex models plots. Based on these plots and the fact that every variable in this model is present in the more complex model, I decide that the assumptions don't need to be investigated further here.

### Model comparison

I check The Aikake criterion

Simple:
```{r}
AIC(m1)
```

Complex:
```{r}
AIC(m2_2)
```

It is very close but the more complex model seems better.

Lets compare the two model with a log-likelihood test.

```{r}
loglike <- lmtest::lrtest(m2_2, m1)
tidy(loglike)
```











